#占用cup核心数
[global]
max_process = 1

#堆栈监控
[profiling]
enable = true
host = 0.0.0.0
port = 6062

#内部数据监控
[metric]
enable = false
host = 0.0.0.0
port = 6063
path = /metrics

[btc]
#是否开启 btc 数据splitter
enable = true
#是否开启数据库
database_enable = true
#数据库worker缓存大小
database_worker_buffer = 8192
#数据库worker数量
database_worker_number = 1
#一次请求区块链数据的最大块数,400000块之前可以设置大一些，比如300
max_batch_block = 30
#btc全节点的地址 
endpoint = [btc 全节点的ip/域名]:[btc 全节点运行端口]
#运行 btc 全节点设置的用户名, 没有就为空
user =
#运行 btc 全节点设置的密码, 没有就为空
password =
#btc数据校验规则文件地址
json_schema_file = /etc/bds-splitter/schema/btc.json↩
#btc数据校验是否开启
json_schema_validation_enable = false

#btc定时任务配置
[cron.btc]
update_meta_expr = @every 1m

#btc kafka 配置
[kafka.btc]
enable = true
topic = btc
# kafka 客户端标示
client_id = btc-client-1
# kafka 消费组标示
group_id = btc-group
# confluent 服务的地址
broker_list = [confluent 服务的ip/域名]:[confluent 服务的运行端口] 
buffer_size = 1000
return_errors = true

#btc数据库配置
[database.btc]
#数据库类型，sql server为mssql，postgre为postgres
type = mssql
#数据库的访问地址
host = [数据库服务的ip/域名] 
#数据库的端口信息
port = [数据库服务的端口] 
#数据库的库名，需要初始化好，创建表和导入数据用
database = [数据库服务的库] 
#数据库的访问账号
user = [数据库服务的账号] 
#数据库的访问账号密码信息
password = [数据库服务的密码]
timezone = Asia/Shanghai
max_open_conns = 500
max_idle_conns = 100
sql_log_file = /var/log/bds-splitter/btc-sql.log
debug = false

#普通日志配置
[logging_normal]
writers = console,file
log_file = /var/log/bds-splitter/normal.log
log_level = debug
caller_level_skip = 5
format = short
max_size = 100m
daily = true

#错误日志配置
[logging_detail]
writers = console,file
log_file = /var/log/bds-splitter/detail.log
log_level = debug
caller_level_skip = 5
format = long
max_size = 100m
daily = true
